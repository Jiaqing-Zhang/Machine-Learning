{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZreMmZ50Gwm"
   },
   "source": [
    "# Supervised Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLiMCXTQ0Gw9"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0pBTm8l0Gxh"
   },
   "source": [
    "# Part 1: Regression on California Test Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O72kduoy0Gxm"
   },
   "source": [
    "## 1. Find the url for the California Test Score Data Set from the following website: https://vincentarelbundock.github.io/Rdatasets/datasets.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "abXlwBCg0Gxr",
    "outputId": "2fb197fa-b13c-4ee9-ffe0-81369a36065a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Caschool.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rw4GFzkL0GyA"
   },
   "source": [
    "## 1.1 Visualize the univariate distribution of the target feature and each of the three continuous explanatory variables that you think are likely to have a relationship with the target feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "kDd0T4fd0GyG",
    "outputId": "9e2d7498-c5b4-432f-f421-b3f8df764b51"
   },
   "outputs": [],
   "source": [
    "df['testscr'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "eNmPHhjf0GyW",
    "outputId": "84c9bff7-cf11-4451-f86c-5282ae8bc8fb"
   },
   "outputs": [],
   "source": [
    "df['expnstu'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "VLJVFgVJ0Gyv",
    "outputId": "b21fb0a3-4296-406e-82bf-590ac5bf4c47"
   },
   "outputs": [],
   "source": [
    "df['str'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "ySHB2TAi0GzA",
    "outputId": "16f5cd59-1ef4-49e9-88f7-9c44e76b76cd"
   },
   "outputs": [],
   "source": [
    "df['avginc'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulJoj4ah0GzT"
   },
   "source": [
    "## 1.2 Visualize the dependency of the target on each feature from 1.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "E3F2v49_0Gzf",
    "outputId": "790cc66b-46dd-482a-b95f-0615d565d424"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "x = sns.regplot(x=\"testscr\", y=\"expnstu\", data=df, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "myiREhAW0Gzv",
    "outputId": "aad8f442-0ceb-4989-c30c-8058bc1d3003"
   },
   "outputs": [],
   "source": [
    "x = sns.regplot(x=\"testscr\", y=\"str\", data=df, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "QrLkZq-k0Gz-",
    "outputId": "092e2785-987e-4172-c06a-9c55456ab450"
   },
   "outputs": [],
   "source": [
    "x = sns.regplot(x=\"testscr\", y=\"avginc\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yvDCB5lJ0G0O"
   },
   "source": [
    "## 1.3a Split data in training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "B0BR4Lj40G0S",
    "outputId": "4d021b48-a4b1-42dd-d16a-b87889b5c9a6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corrected_data = df.drop(df.columns[[16, 17]], axis=1) # remove the reading and math scores from the dataset\n",
    "data = corrected_data.iloc[:, 5:] # delete first six columns\n",
    "\n",
    "y = data['testscr'] # assign outcome variable, i.e. what we want to predict\n",
    "X = data.loc[:, data.columns != 'testscr'] # assign all other values as X-values, i.e. variables we use to predict\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "# randomly assign some data to the test-set and the rest to the training-set\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EVqKb37e0G0j"
   },
   "source": [
    "## 1.3b Build models that evaluate the relationship between all available X variables in the California test dataset and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlYh_mSX0G0n"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Gov_SQE0G0z"
   },
   "outputs": [],
   "source": [
    "# Set up function parameters for different cross validation strategies\n",
    "kfold = KFold(n_splits=5) # I use this in PART 1.\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True) \n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=10, random_state=1) # I use this in PART 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A18P0l5H0G0_"
   },
   "source": [
    "### KNN for regression with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "bqpT_Mu_0G1C",
    "outputId": "3f7e3366-b3e1-4e08-8cfb-77087eb2d5bf"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=14) # Play around with n_neighbors to determine best fit\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"KNN for REGRESSION (UNSCALED DATA)\")\n",
    "\n",
    "# Training and Test Scores\n",
    "print(\"Training set score: {:.2f}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "\n",
    "# Kfold cross validation\n",
    "print(\"Mean Cross-Validation, Kfold: {:.2f}\".format(np.mean(cross_val_score(knn, X_train, y_train, scoring='r2', cv=kfold))))\n",
    "\n",
    "knn_unscaled = np.mean(cross_val_score(knn, X_train, y_train, scoring='r2', cv=kfold)) # Will use later to compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9tfjKQZ0G1P"
   },
   "source": [
    "**Note:** KNN Regression with unscaled data is not accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bG4SZQAs0G1R"
   },
   "source": [
    "### Linear Regression (OLS) with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "5HPUUHd10G1U",
    "outputId": "41f2fabc-6958-4eaa-f9bf-1b1a3be45b0b"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train) # Fit the training data to a regression line\n",
    "\n",
    "print(\"LINEAR REGRESSION (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
    "\n",
    "# Kfold cross validation\n",
    "print(\"Mean Cross-Validation, Kfold: {:.2f}\".format(np.mean(cross_val_score(lr, X_train, y_train, cv=kfold))))\n",
    "\n",
    "OLS_unscaled = np.mean(cross_val_score(lr, X_train, y_train, cv=kfold)) # Will use later to compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qR07Odx_0G1m"
   },
   "source": [
    "**Note:** Linear Regression with unscaled data has an avg. accuracy of 78%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PA91Qu4p0G1p"
   },
   "source": [
    "### Ridge with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "Ug_DTTrT0G1r",
    "outputId": "d5792ee8-104f-4e5d-b4bf-f42e47e96bca"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=0.01, max_iter=100000).fit(X_train, y_train) # Fit the training data to a ridge regression line\n",
    "\n",
    "print(\"RIDGE REGRESSION (UNSCALED DATA)\") # Ridge is L2 penalty, which adds “squared magnitude” of coefficient as penalty term to the loss function. Good for avoiding the over-fitting issue\n",
    "print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\n",
    "\n",
    "# Kfold cross validation\n",
    "print(\"Mean Cross-Validation, Kfold: {:.2f}\".format(np.mean(cross_val_score(ridge, X_train, y_train, cv=kfold))))\n",
    "\n",
    "ridge_unscaled = np.mean(cross_val_score(ridge, X_train, y_train, cv=kfold)) # Will use later to compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4spCNmc0G16"
   },
   "source": [
    "**Note:** Ridge Regression with unscaled data has an avg. accuracy of 78%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwUqr6U40G19"
   },
   "source": [
    "### LASSO with cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "yhgYccwq0G2B",
    "outputId": "26d21b2c-8c41-4775-f602-218fb3470bbb"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train) # Fit the training data to a lasso regression line with alpha = 0.01 and 100,000 iterations\n",
    "\n",
    "print(\"LASSO REGRESSION (UNSCALED DATA)\") # LASSO is L1 penalty, which adds “absolute value of magnitude” of coefficient as penalty term to the loss function. Good for feature selection when you have a lot of features.\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "\n",
    "# Kfold cross validation\n",
    "print(\"Mean Cross-Validation, Kfold: {:.2f}\".format(np.mean(cross_val_score(lasso, X_train, y_train, cv=kfold))))\n",
    "\n",
    "lasso_unscaled = np.mean(cross_val_score(lasso, X_train, y_train, cv=kfold)) # Will use later to compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izvVHV9N0G2K"
   },
   "source": [
    "**Note:** Lasso regression with unscaled data as the same avg. accuracy as Ridge, 78%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCt2wqpC0G2N"
   },
   "source": [
    "## 1.3c Does scaling the data with the StandardScaler help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmhzW70z0G2T"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Here, I standardize by X data using StandardScalar\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DF5DX4K10G2c"
   },
   "source": [
    "### Scaled KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "X45LXwzE0G2e",
    "outputId": "966703f9-97b3-4285-e451-bcfbb4fc08cb"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=14).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN for REGRESSION (SCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(knn.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Kfold cross validation\n",
    "print(\"Mean Cross-Validation, Kfold: {:.2f}\".format(np.mean(cross_val_score(knn, X_train_scaled, y_train, cv=kfold))))\n",
    "\n",
    "knn_scaled = np.mean(cross_val_score(knn, X_train_scaled, y_train, cv=kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghvhujHh0G2o"
   },
   "source": [
    "**Note:** Avg. accuracy of KNN model improves greatly with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-wPvvciL0G2r",
    "outputId": "f0463f9d-6787-4a36-865b-99dc339294ce"
   },
   "outputs": [],
   "source": [
    "print(\"KNN with unscaled data: {:.2f}\".format(knn_unscaled))\n",
    "print(\"KNN with scaled data  : {:.2f}\".format(knn_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBOyTAF20G2z"
   },
   "source": [
    "### Scaled Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "CWIuciev0G21",
    "outputId": "77c30b5c-99e1-4ae3-8874-6a9c34dcfec1"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"LINEAR REGRESSION (SCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Kfold cross validation\n",
    "print(\"Mean Cross-Validation, Kfold: {:.2f}\".format(np.mean(cross_val_score(lr, X_train_scaled, y_train, cv=kfold))))\n",
    "\n",
    "OLS_scaled = np.mean(cross_val_score(lr, X_train_scaled, y_train, cv=kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLCu5Xz30G28"
   },
   "source": [
    "**Note:** Avg. accuracy of linear regression model does not improve with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "A97j4BHm0G2-",
    "outputId": "b9ce019c-1aab-49e5-c749-33d218cc117f"
   },
   "outputs": [],
   "source": [
    "print(\"Linear regression with unscaled data: {:.2f}\".format(OLS_unscaled))\n",
    "print(\"Linear regression with scaled data  : {:.2f}\".format(OLS_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eW2I1eT0G3J"
   },
   "source": [
    "### Scaled Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "SyNIws9R0G3O",
    "outputId": "18f5d203-340f-4c47-8ebd-17be9c284855"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=0.01, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"RIDGE REGRESSION (SCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(ridge.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Kfold cross validation\n",
    "print(\"Mean Cross-Validation, Kfold: {:.2f}\".format(np.mean(cross_val_score(ridge, X_train_scaled, y_train, cv=kfold))))\n",
    "\n",
    "ridge_scaled = np.mean(cross_val_score(ridge, X_train_scaled, y_train, cv=kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFoQLjyE0G3b"
   },
   "source": [
    "**Note:** Avg. accuracy of ridge model does not improve with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "LyX-cGq30G3e",
    "outputId": "1fc220e4-5b4e-426d-e2c2-2c84291c815f"
   },
   "outputs": [],
   "source": [
    "print(\"Ridge regression with unscaled data: {:.2f}\".format(ridge_unscaled))\n",
    "print(\"Ridge regression with scaled data  : {:.2f}\".format(ridge_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AiPiS8-P0G3l"
   },
   "source": [
    "### Scaled Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "MOHoWC810G3n",
    "outputId": "90793289-c0e6-4a53-d5b7-e1623bb86dda"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.01, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"LASSO REGRESSION (SCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Kfold cross validation\n",
    "print(\"Mean Cross-Validation, Kfold: {:.2f}\".format(np.mean(cross_val_score(lasso, X_train_scaled, y_train, cv=kfold))))\n",
    "\n",
    "lasso_scaled = np.mean(cross_val_score(lasso, X_train_scaled, y_train, cv=kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeukElb40G3y"
   },
   "source": [
    "**Note:** Avg. accuracy of LASSO model does not improve with scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "X0Aef53o0G3z",
    "outputId": "1fc65917-4014-485b-b534-76eff1b0ae6c"
   },
   "outputs": [],
   "source": [
    "print(\"LASSO regression with unscaled data: {:.2f}\".format(lasso_unscaled))\n",
    "print(\"LASSO regression with scaled data  : {:.2f}\".format(lasso_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsrirmrL0G37"
   },
   "source": [
    "**Answer:** \n",
    "Scalling improves training scores for KNN, but affords little change to the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KbAPAmAY0G3-"
   },
   "source": [
    "## 1.4 Tune the parameters of the models where possible using GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSsiGXhb0G4A"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Z7UeZ5f0G4I"
   },
   "source": [
    "### GridsearchCV with KNN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4T6-loqx0G4J",
    "outputId": "374b038e-314c-4f24-e144-d1fafd26ee61"
   },
   "outputs": [],
   "source": [
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "# print(knn_pipe.steps) \n",
    "# Name of step = 'kneighborsregressor' + __ + n_neighbors\n",
    "\n",
    "knn_param_grid = {'kneighborsregressor__n_neighbors': range(1, 10)} # remember, use two underscores before n, \"__n\"\n",
    "knn_grid = GridSearchCV(knn_pipe, knn_param_grid, cv=kfold).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN for REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(knn_grid.score(X_test_scaled, y_test)))\n",
    "#print(\"Best Cross-Validation Score: {:.2f}\".format(knn_grid.best_score_))\n",
    "print(\"Best Parameter: {}\".format(knn_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwIDR4lT0G4Q"
   },
   "source": [
    "**Note:** Best parameter for KNN is n_neighbors = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAnDcWNm0G4V",
    "outputId": "7c785e1c-d82e-46fe-e960-f6a58037d4e1"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=6).fit(X_train_scaled, y_train)\n",
    "print(\"KNN Test set score: {:.2f}\".format(knn.score(X_test_scaled, y_test)))\n",
    "best_knn = knn.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8siJ_lwp0G4b"
   },
   "source": [
    "### GridsearchCV with Linear Regression (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BQlHqpfO0G4d"
   },
   "source": [
    "**Note:** There are no parameters to 'tune' with OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aNKILFcQ0G4f"
   },
   "source": [
    "### Gridsearch with Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3Uqrpdf0G4g",
    "outputId": "c7ca4666-cd14-44fb-e05f-89bef0945a51"
   },
   "outputs": [],
   "source": [
    "ridge_pipe = make_pipeline(StandardScaler(), Ridge())\n",
    "# print(ridge_pipe.steps) \n",
    "# Name of step = 'ridge' + __ + alpha\n",
    "\n",
    "ridge_param_grid = {'ridge__alpha': np.linspace(0.1, 100.1, 1)}\n",
    "ridge_grid = GridSearchCV(ridge_pipe, ridge_param_grid, cv=kfold).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"RIDGE REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(ridge_grid.score(X_test_scaled, y_test)))\n",
    "#print(\"Best Cross-Validation Score: {:.2f}\".format(ridge_grid.best_score_))\n",
    "print(\"Best Parameter: {}\".format(ridge_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uLwBXqB60G4n"
   },
   "source": [
    "**Note:** Best parameter for ridge regression is alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qrkw-sa90G4p",
    "outputId": "0bc41988-127a-4e84-b3f4-4cf479752719"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=0.1, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "print(\"Ridge test set score: {:.2f}\".format(ridge.score(X_test_scaled, y_test)))\n",
    "best_ridge = np.mean(cross_val_score(ridge, X_train_scaled, y_train, cv=kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fk0gohGH0G4v"
   },
   "source": [
    "### Gridsearch with LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLFA_kRl0G4x",
    "outputId": "11a8f700-e2c6-490c-b999-ed1bd100a00d"
   },
   "outputs": [],
   "source": [
    "lasso_pipe = make_pipeline(StandardScaler(), Lasso())\n",
    "# print(lasso_pipe.steps) \n",
    "# Name of step = 'lasso' + __ + alpha\n",
    "\n",
    "lasso_param_grid = {'lasso__alpha': np.linspace(0.1, 100.0, 1)}\n",
    "lasso_grid = GridSearchCV(lasso_pipe, lasso_param_grid, cv=kfold).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"LASSO REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(lasso_grid.score(X_test_scaled, y_test)))\n",
    "#print(\"Best Cross-Validation Score: {:.2f}\".format(lasso_grid.best_score_))\n",
    "print(\"Best Parameter: {}\".format(lasso_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BExDCMuh0G47"
   },
   "source": [
    "**Note:** Best parameter for lasso regression is alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wrk_bUmS0G49",
    "outputId": "cee68291-4f7e-433e-c682-d6f9afe78b0c"
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.1, max_iter=100000).fit(X_train_scaled, y_train)\n",
    "print(\"LASSO test set score: {:.2f}\".format(lasso.score(X_test_scaled, y_test)))\n",
    "best_lasso = np.mean(cross_val_score(lasso, X_train_scaled, y_train, cv=kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHK6x8AM0G5G"
   },
   "source": [
    "## Do the results improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KjhdDnlu0G5I",
    "outputId": "ec66ad24-94db-41d6-9561-48fcb288850f"
   },
   "outputs": [],
   "source": [
    "print(\"KNN\")\n",
    "print(\"Before Gridsearch: {:.2f}\".format(knn_scaled))\n",
    "print(\"After Gridsearch: {:.2f}\".format(best_knn))\n",
    "print(\"\")\n",
    "print(\"RIDGE\")\n",
    "print(\"Before Gridsearch: {:.2f}\".format(ridge_scaled))\n",
    "print(\"After Gridsearch: {:.2f}\".format(best_ridge))\n",
    "print(\"\")\n",
    "print(\"LASSO\")\n",
    "print(\"Before Gridsearch: {:.2f}\".format(lasso_scaled))\n",
    "print(\"After Gridsearch: {:.2f}\".format(best_lasso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8AYmXP80G5T"
   },
   "source": [
    "**Answer:** Yes. Tuning the parameters slightly improved each model, but KNN showed the only improvement that's really worthy of note."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EbvJf7yI0G5V"
   },
   "source": [
    "## 1.5 Compare the coefficients of your two best linear models (not knn), do they agree on which features are important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNg8ck0L0G5a"
   },
   "source": [
    "**Note:** My two best linear models are Ridge and LASSO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkwMc3S80G5e",
    "outputId": "9d3015b3-e975-41c5-d262-faa5e4b97fc8"
   },
   "outputs": [],
   "source": [
    "d = {'LASSO': lasso.coef_, 'Ridge': ridge.coef_} # Builds a dictionary with the coefficients from my LASSO & Ridge models\n",
    "coefficients = pd.DataFrame(data=d, index=X.columns) # Puts the dictionary into a dataframe along with variable names\n",
    "coefficients.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48RHTNkh0G5n"
   },
   "source": [
    "**Answer:** LASSO restricts (or shrinks) the less influential features to zero (see evidence in the code below). In this case, my LASSO has converted features with coefficients approximately between 0.5 and -0.5 to zero. Still, LASSO and Ridge agree on which features are important. For example, both models agree that **'mealpct'** has a negative relationship with 'testscr', while **'expnstu'** and **'anginc'** have a positive relationship with 'testscr'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SRcixER0G5o",
    "outputId": "4f5105e5-e56f-4f58-d674-ea3c94407960"
   },
   "outputs": [],
   "source": [
    "print(\"Number of features used with LASSO: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "print(\"Number of features used with Ridge: {}\".format(np.sum(ridge.coef_ != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DuKR-2Wt0G5v"
   },
   "source": [
    "## 1.6 Discuss which final model you would choose to predict new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vclfvlAj0G5w"
   },
   "source": [
    "**Answer:** While each model has its advantages, I recommend LASSO with alpha = 0.1.  It has a slightly higher cross validated accuracy than other models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRS2hD6M0G5y"
   },
   "source": [
    "# Part 2: Classification on red and white wine characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_nZNfvh0G50"
   },
   "source": [
    "First, import the red and the white wine csv files into separate pandas dataframes from the following website:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/Links to an external site.\n",
    "\n",
    "(Note: you need to adjust the argument for read_csv() from sep=',' to sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnd5BZe80G55",
    "outputId": "ac3125db-0296-405a-ee2c-abdd1abdd46f"
   },
   "outputs": [],
   "source": [
    "r = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep = ';')\n",
    "r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMfx-dIV0G6W",
    "outputId": "c6c0f5e9-e834-4346-8b1c-2b2873caedc9"
   },
   "outputs": [],
   "source": [
    "w = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", sep = ';')\n",
    "w.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQmmXvCm0G6e"
   },
   "source": [
    "Add a new column to each data frame called \"winetype\".  For the white wine dataset label the values in this column with a 0, indicating white wine.  For the red wine dataset, label values with a 1, indicating red wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mkk3WNcY0G6g",
    "outputId": "488d97b5-c490-4589-f0dd-b1fa7ba79e95"
   },
   "outputs": [],
   "source": [
    "red = r.assign(winetype=1)\n",
    "red.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5XkPcJ00G6p",
    "outputId": "79904610-7e2e-4d29-d7fc-e5cfe90baead"
   },
   "outputs": [],
   "source": [
    "white = w.assign(winetype=0)\n",
    "white.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bso43QtC0G6w"
   },
   "source": [
    "Combine both datasets into a single dataframe.\n",
    "The target data (i.e. the dependent variable) is \"winetype\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlwd1zTO0G6x",
    "outputId": "3633044a-4c62-46b9-ddd0-1a093ad16800"
   },
   "outputs": [],
   "source": [
    "mix = [white, red]\n",
    "wines = pd.concat(mix)\n",
    "wines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PmNMrhB0G66"
   },
   "source": [
    "## 2.1 Visualize the univariate distribution of the target feature and each of the three explanatory variables that you think are likely to have a relationship with the target feature.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "us72_zvy0G68",
    "outputId": "820cdcca-57fd-4977-a76c-6050a3891fa4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(x='winetype', y='fixed acidity', data=wines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gdWFFkOT0G7C",
    "outputId": "103a0151-4594-4a49-c95a-ca2ceac3c3d0"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='winetype', y='citric acid', data=wines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "05BmAdK-0G7U",
    "outputId": "322af2d0-62e4-4483-eb89-6bce718fe39b"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='winetype', y='total sulfur dioxide', data=wines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kMDXcdWu0G7b"
   },
   "source": [
    "## 2.2a Split data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qpo-tzlC0G7e",
    "outputId": "ad57aa0e-8c03-4f29-85c3-7b09afb6038e"
   },
   "outputs": [],
   "source": [
    "y = wines['winetype']\n",
    "X = wines.loc[:, data.columns != 'winetype']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) \n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eo-SV6Ln0G7i"
   },
   "source": [
    "## 2.2b Build models that evaluate the relationship between all available X variables in the dataset and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mx2FNDnC0G7j"
   },
   "source": [
    "### Logistic Regression using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3P6Y1_80G7l",
    "outputId": "1bb458f9-9911-414c-cba6-98b5737c5ad8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) # I did this to remove all the warnings :)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test))) \n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.2f}\".format(np.mean(cross_val_score(logreg, X_train, y_train, cv=kfold))))\n",
    "\n",
    "# Wine type prediction from test set (I'll use this later)\n",
    "logreg_predicted_vals = logreg.predict(X_test)\n",
    "\n",
    "# Organize the model coefficients\n",
    "logreg_coef = pd.DataFrame(data=logreg.coef_, columns=X.columns, index=['Logistic Regression Coefficients'])\n",
    "logreg_coef.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQ54ec9Z0G7q"
   },
   "source": [
    "### Penalized Logistic Regression using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oaZtzW3B0G7s",
    "outputId": "aeee7aad-70c5-4142-d9a0-7954dff9dda7"
   },
   "outputs": [],
   "source": [
    "pen_logreg = LogisticRegression(penalty = 'l1').fit(X_train, y_train) # Default = L2, Penalty = L1\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(pen_logreg.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(pen_logreg.score(X_test, y_test)))\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.2f}\".format(np.mean(cross_val_score(pen_logreg, X_train, y_train, cv=kfold))))\n",
    "\n",
    "# Wine type prediction from test set (I'll use this later)\n",
    "pen_logreg_predicted_vals = pen_logreg.predict(X_test)\n",
    "\n",
    "# Organize the model coefficients\n",
    "pen_logreg_coef = pd.DataFrame(data=pen_logreg.coef_, columns=X.columns, index=['Penalized Logistic Regression Coefficients'])\n",
    "\n",
    "# Print coefficients for comparison\n",
    "c = [logreg_coef, pen_logreg_coef]\n",
    "coefs = pd.concat(c)\n",
    "coefs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbUrlRP70G70"
   },
   "source": [
    "### KNN Classifier using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlfTN0e50G71",
    "outputId": "6e819c35-471d-4170-fef9-bd4e36b8a441"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Notice that I'm using a Classifier here (as opposed to the Regressor we used in PART 1)\n",
    "\n",
    "knn = KNeighborsClassifier().fit(X_train, y_train)\n",
    "\n",
    "print(\"KNN CLASSIFER (UNSCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(knn.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.2f}\".format(np.mean(cross_val_score(knn, X_train, y_train, cv=kfold))))\n",
    "\n",
    "# Wine type prediction from test set (I'll use this later)\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SUbXm78H0G79"
   },
   "source": [
    "**Note:** KNN classifier can not produce coefficients. So we don't include them here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIKwu3hz0G7-"
   },
   "source": [
    "## How different are the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JUzwZ8qC0G8D",
    "outputId": "b93e99c5-3498-4138-8c3d-9bf9596f2565"
   },
   "outputs": [],
   "source": [
    "# Organize predictions from each model into a dataframe\n",
    "results = pd.DataFrame()\n",
    "results = results.assign(L2=logreg_predicted_vals)\n",
    "results = results.assign(L1=pen_logreg_predicted_vals)\n",
    "results = results.assign(knn=y_pred)\n",
    "\n",
    "# Count the combinations\n",
    "print(\"COMBINATIONS OF PREDICTIONS: LOG REG by PEN LOG REG by KNN\")\n",
    "results.groupby(['L2','L1'])['knn'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkKS3EWf0G8P"
   },
   "source": [
    "**Note:** KNN shows the highest count where the Logistic Regression and the Penalized Logistic Regression models agree (0, 0, 1241) and (1, 1, 370)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqoXzLVn0G8R",
    "outputId": "fe3611e5-a65a-4a31-aaea-4b30c5e81d42"
   },
   "outputs": [],
   "source": [
    "results.groupby(['L2','knn'])['L1'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iw3Ykuet0G8a"
   },
   "source": [
    "**Note:** Penalized Regressions shows the highest count where the Logistic Regression and the KNN models agree (0, 0, 1210) and (1, 1, 323). (Notice that Penlized Logistic Regression is less likely to agree with the combined predictions of KNN and Logistic Regression than KNN is to agree with the combined predictions of Logistic Regression and Penalized Regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUwNPK8z0G8c",
    "outputId": "970fb73e-daa2-4047-b7a1-3ccb38190843"
   },
   "outputs": [],
   "source": [
    "results.groupby(['knn','L1'])['L2'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6G43SV9r0G8j"
   },
   "source": [
    "**Note:** Logistic Regressions shows the highest count where the KNN and Penalized Logistic Regression models agree (0, 0, 1205) and (1, 1, 322). (Notice that Logistic  Regression is also less likely to agree with the combined predictions of KNN and Penalized Logistic Regression than KNN is to agree with the combined predictions of Logistic Regression and Penalized Regression. This is a similar pattern noted in the previous cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGaP4y3D0G8k"
   },
   "source": [
    "**Answer:** The regressions are more accurate than the nearest neighbor (Logisic Regression Accuracy = 0.98, Penalized Logistic Regression Accuracy = 0.99, KNN Accuracy = 0.94). This is most likely the result of their predicting slightly different results. Logistic Regression and Penalized Logistic Regression tend to make similar predictions - only disagreeing in about 10 instances - while the KNN Classifier model disagrees with the logistic model predictions by in about 50 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i0DbJ5_I0G8m"
   },
   "source": [
    "## How does scaling the data with StandardScaler influence the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JOY9sjY0G8n"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KuiJd3b20G8t"
   },
   "source": [
    "### Scaled Logistic Regression using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8VWDRo60G8x",
    "outputId": "85cebe2e-7456-453b-f959-8b67ff81b2f2"
   },
   "outputs": [],
   "source": [
    "logreg_scaled = LogisticRegression().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(logreg_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(logreg_scaled.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.2f}\".format(np.mean(cross_val_score(logreg_scaled, X_train_scaled, y_train, cv=kfold))))\n",
    "\n",
    "# Organize the model coefficients\n",
    "logreg_scaled_coef = pd.DataFrame(data=logreg_scaled.coef_, columns=X.columns, index=['Scaled Logistic Regression Coefficients'])\n",
    "\n",
    "# Print coefficients for comparison\n",
    "c2 = [logreg_coef, pen_logreg_coef, logreg_scaled_coef]\n",
    "coefs = pd.concat(c2)\n",
    "coefs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oouvnb700G83"
   },
   "source": [
    "### Scaled Penalized Logistic Regression using cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OZmlPF-0G85",
    "outputId": "665d5f29-87ab-40d4-cf59-3f73e5d6310f"
   },
   "outputs": [],
   "source": [
    "pen_logreg_scaled = LogisticRegression(penalty = 'l1').fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENALIZED LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(pen_logreg_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(pen_logreg_scaled.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Stratified Kfold Cross Validation \n",
    "print(\"Mean Cross Validation, KFold: {:.2f}\".format(np.mean(cross_val_score(pen_logreg_scaled, X_train, y_train, cv=kfold))))\n",
    "\n",
    "# Organize the model coefficients\n",
    "pen_logreg_scaled_coef = pd.DataFrame(data=pen_logreg_scaled.coef_, columns=X.columns, index=['Scaled Penalized Logistic Regression Coefficients'])\n",
    "\n",
    "# Print coefficients for comparison\n",
    "c3 = [logreg_coef, pen_logreg_coef, logreg_scaled_coef, pen_logreg_scaled_coef]\n",
    "coefs = pd.concat(c3)\n",
    "coefs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "settEjOY0G9B"
   },
   "source": [
    "### Scaled KNN using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15_-VPcQ0G9C",
    "outputId": "d74a692d-f439-432d-80f2-cb5e886403d5"
   },
   "outputs": [],
   "source": [
    "knn_scaled = KNeighborsRegressor().fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN CLASSIFER (SCALED DATA)\")\n",
    "print(\"Training set score: {:.2f}\".format(knn_scaled.score(X_train_scaled, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(knn_scaled.score(X_test_scaled, y_test)))\n",
    "\n",
    "# Stratified Kfold Cross Validation\n",
    "print(\"Mean Cross Validation, KFold: {:.2f}\".format(np.mean(cross_val_score(knn_scaled, X_train_scaled, y_train, cv=kfold))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9GqXJik0G9I"
   },
   "source": [
    "**Note:** KNN classifier can not produce coefficients. So we don't include them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RD669bJX0G9L",
    "outputId": "736642bc-c9fe-454a-f889-fe6fab566dd9"
   },
   "outputs": [],
   "source": [
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Unscaled: {:.2f}\".format(logreg.score(X_test, y_test)))\n",
    "print(\"Scaled: {:.2f}\".format(logreg_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"\")\n",
    "print(\"PENALIZED LOGISTIC REGRESSION\")\n",
    "print(\"Unscaled: {:.2f}\".format(pen_logreg.score(X_test, y_test)))\n",
    "print(\"Scaled: {:.2f}\".format(pen_logreg_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"\")\n",
    "print(\"KNN CLASSIFIER\")\n",
    "print(\"Unscaled: {:.2f}\".format(knn.score(X_test, y_test)))\n",
    "print(\"Scaled: {:.2f}\".format(knn_scaled.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5iqIWstu0G9Q"
   },
   "source": [
    "**Answer:** Scalling the data with StandardScaler improves the results for all models, particularly for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ci8XsBXl0G9S"
   },
   "source": [
    "## 2.3 Tune the parameters where possible using GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30ES52Op0G9T"
   },
   "source": [
    "### GridsearchCV with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSmNyNhX0G9V",
    "outputId": "a4e977d1-bb1e-49b7-f465-6bce5262ecac"
   },
   "outputs": [],
   "source": [
    "logreg_pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "#print(logreg_pipe.steps) \n",
    "# Name of step = 'logisticregression' + __ + C\n",
    "\n",
    "logreg_param_grid = {'logisticregression__C': np.linspace(1, 100, 100)}\n",
    "logreg_grid = GridSearchCV(logreg_pipe, logreg_param_grid).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(logreg_grid.score(X_test_scaled, y_test)))\n",
    "print(\"Best Parameter: {}\".format(logreg_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2eqH91Vw0G9g"
   },
   "source": [
    "### GridsearchCV with Penalized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6DCYjgr0G9k",
    "outputId": "e4ae5c97-c6dd-4ceb-f4cb-b0a737a55fe6"
   },
   "outputs": [],
   "source": [
    "pen_logreg_pipe = make_pipeline(StandardScaler(), LogisticRegression(penalty='l1'))\n",
    "#print(pen_logreg_pipe.steps) \n",
    "# Name of step = 'logisticregression' + __ + C\n",
    "\n",
    "pen_logreg_param_grid = {'logisticregression__C': np.linspace(1, 100, 100)}\n",
    "pen_logreg_grid = GridSearchCV(pen_logreg_pipe, pen_logreg_param_grid).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENLIZED LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(pen_logreg_grid.score(X_test_scaled, y_test)))\n",
    "print(\"Best Parameter: {}\".format(pen_logreg_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Ww5G-Ue0G9s"
   },
   "source": [
    "### GridsearchCV with KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfa_jCXV0G9u",
    "outputId": "b7bee378-7cd3-4f3c-aafd-74b00cefe113"
   },
   "outputs": [],
   "source": [
    "knn_pipe = make_pipeline(StandardScaler(), KNeighborsClassifier())\n",
    "\n",
    "knn_param_grid = {'kneighborsclassifier__n_neighbors': range(1, 10)}\n",
    "knn_grid = GridSearchCV(knn_pipe, knn_param_grid).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN for REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(knn_grid.score(X_test_scaled, y_test)))\n",
    "print(\"Best Parameter: {}\".format(knn_grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KsjiPJ20G9z"
   },
   "source": [
    "## Do the results improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcE-FCOz0G90",
    "outputId": "e175a5df-6dc2-4347-c487-ef214986a087"
   },
   "outputs": [],
   "source": [
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Before Gridsearch: {:.2f}\".format(logreg_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"After Gridsearch: {:.2f}\".format(logreg_grid.score(X_test_scaled, y_test)))\n",
    "print(\"\")\n",
    "print(\"PENALIZED LOGISTIC REGRESSION\")\n",
    "print(\"Before Gridsearch: {:.2f}\".format(pen_logreg_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"After Gridsearch: {:.2f}\".format(pen_logreg_grid.score(X_test_scaled, y_test)))\n",
    "print(\"\")\n",
    "print(\"KNN CLASSIFIER\")\n",
    "print(\"Before Gridsearch: {:.2f}\".format(knn_scaled.score(X_test_scaled, y_test)))\n",
    "print(\"After Gridsearch: {:.2f}\".format(knn_grid.score(X_test_scaled, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wVYguba0G96"
   },
   "source": [
    "**Answer:** Parameters recommended by Gridsearch make noticable improvements to the accuracy score for the KNN model, but new parameters recommended by Gridsearch do not a remarakable difference for the accuracy scores of the Logistic or Penalized Logistic Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6CdFXZq0G98"
   },
   "source": [
    "## 2.4 Change the cross-validation strategy in GridSearchCV from ‘stratified k-fold’ to ‘kfold’ with shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnwU5UFQ0G9-"
   },
   "outputs": [],
   "source": [
    "# Note: See code above for kfold, skfold, and rkf. \n",
    "# Here, I am going to use the rkf code.\n",
    "# rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5JNNPeE0G-D"
   },
   "source": [
    "**Note:** For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IcafXfpL0G-G"
   },
   "source": [
    "Because our data uses 'winetype' as a target, our data qualifies as binary/multiclass. This means that StratigiedKFold (skfold) will be a default of GridsearchCV. To change this default setting and to include shuffling, we need to set cv= Repeated KFold (rkf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRy8Di1U0G-I"
   },
   "source": [
    "### GridsearchCV of Logistic Regression with Repeated KFold (rkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7fVh9hU20G-K",
    "outputId": "bbe23455-6f6b-4d3a-ba69-cdf8f3157545"
   },
   "outputs": [],
   "source": [
    "logreg_grid_rfk = GridSearchCV(logreg_pipe, logreg_param_grid, cv=rkf).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(logreg_grid_rfk.score(X_test_scaled, y_test)))\n",
    "print(\"Best Parameter: {}\".format(logreg_grid_rfk.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N5utqSlQ0G-Q"
   },
   "source": [
    "### GridsearchCV of Penalized Logistic Regression with Repeated KFold (rkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r27sHy_Z0G-R",
    "outputId": "a0b48d88-f250-407d-a227-07bb7d8ecfe7"
   },
   "outputs": [],
   "source": [
    "pen_logreg_grid_rfk = GridSearchCV(pen_logreg_pipe, pen_logreg_param_grid, cv=rkf).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENLIZED LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(pen_logreg_grid_rfk.score(X_test_scaled, y_test)))\n",
    "print(\"Best Parameter: {}\".format(pen_logreg_grid_rfk.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9SFNPOl0G-X"
   },
   "source": [
    "### GridsearchCV of KNN Classifier with Repeated KFold (rkf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Os3voMFP0G-Y",
    "outputId": "1f4053fb-4b6e-4c7e-cc52-54799bcdd7cd"
   },
   "outputs": [],
   "source": [
    "knn_grid_rfk = GridSearchCV(knn_pipe, knn_param_grid, cv=rkf).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN for REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(knn_grid_rfk.score(X_test_scaled, y_test)))\n",
    "print(\"Best Parameter: {}\".format(knn_grid_rfk.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wKTIpJvE0G-m"
   },
   "source": [
    "## Do the parameters for models that can be tuned change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRIHO2QK0G-n",
    "outputId": "b6ac5476-eb8c-4d51-cacd-f0b87e5b417f"
   },
   "outputs": [],
   "source": [
    "# Comparing parameters with and without Kfold shuffling\n",
    "print(\"Parameters for LOGISTIC REGRESSION\")\n",
    "print(\"KFold w/out Shuffling: {}\".format(logreg_grid.best_params_))\n",
    "print(\"       Repeated KFold: {}\".format(logreg_grid_rfk.best_params_))\n",
    "print(\"\")\n",
    "print(\"Parameters for PENALIZED LOGISTIC REGRESSION\")\n",
    "print(\"KFold w/out Shuffling: {}\".format(pen_logreg_grid.best_params_))\n",
    "print(\"       Repeated KFold: {}\".format(pen_logreg_grid_rfk.best_params_))\n",
    "print(\"\")\n",
    "print(\"Parameters for KNN CLASSIFIER\")\n",
    "print(\"KFold w/out Shuffling: {}\".format(knn_grid.best_params_))\n",
    "print(\"       Repeated KFold: {}\".format(knn_grid_rfk.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f-_ET6ns0G-0"
   },
   "source": [
    "**Answer:** Yes. After changing the cross validation method from skfold to kfold, the parameters recommended by gridsearchCV are C=28 for the Logistic Regression (previously C=2), and n_neighbors=6 for KNN Classifier (previously n_neighbors=2). Gridsearch does not recommend different parameters for the Penalized Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TK8hSr3p0G-1"
   },
   "source": [
    "## Do they change if you change the random seed of the shuffling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vy9LVJCN0G-3"
   },
   "outputs": [],
   "source": [
    "# Change random_state=None to random_state=1\n",
    "rkf_1 = RepeatedKFold(n_splits=5, n_repeats=10, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YF-vz7j90G-7"
   },
   "source": [
    "### GridsearchCV of Logistic Regression with Repeated KFold (rkf) and new random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5IwY1xty0G-9",
    "outputId": "b4810ad3-6308-42a6-d75e-53d4d12f279c"
   },
   "outputs": [],
   "source": [
    "logreg_grid_rfk_1 = GridSearchCV(logreg_pipe, logreg_param_grid, cv=rkf_1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(logreg_grid_rfk_1.score(X_test_scaled, y_test)))\n",
    "print(\"Best Parameter: {}\".format(logreg_grid_rfk_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwJ4bX7s0G_D"
   },
   "source": [
    "### GridsearchCV of Penalized Logistic Regression with Repeated KFold (rkf) and new random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "52gHmRjW0G_F",
    "outputId": "e202901b-221e-4666-e4c3-9c6cd0373f87"
   },
   "outputs": [],
   "source": [
    "pen_logreg_grid_rfk_1 = GridSearchCV(pen_logreg_pipe, pen_logreg_param_grid, cv=rkf_1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"PENLIZED LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(pen_logreg_grid_rfk_1.score(X_test_scaled, y_test)))\n",
    "print(\"Best Parameter: {}\".format(pen_logreg_grid_rfk_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2W_STPVK0G_I"
   },
   "source": [
    "### GridsearchCV of KNN Classifier with Repeated KFold (rkf) and new random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgoZkgVE0G_K",
    "outputId": "ee25602d-2dc2-472d-c720-ff1affa2e631"
   },
   "outputs": [],
   "source": [
    "knn_grid_rfk_1 = GridSearchCV(knn_pipe, knn_param_grid, cv=rkf_1).fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"KNN for REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(knn_grid_rfk_1.score(X_test_scaled, y_test)))\n",
    "print(\"Best Parameter: {}\".format(knn_grid_rfk_1.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmXcarDP0G_P",
    "outputId": "b52afc6e-c625-4cb2-9ebf-7e9bb063a423"
   },
   "outputs": [],
   "source": [
    "# Comparing parameters with new random seed\n",
    "print(\"Parameters for LOGISTIC REGRESSION\")\n",
    "print(\"     KFold w/out Shuffling: {}\".format(logreg_grid.best_params_))\n",
    "print(\"            Repeated KFold: {}\".format(logreg_grid_rfk.best_params_))\n",
    "print(\"Repeated KFold w/ new seed: {}\".format(logreg_grid_rfk_1.best_params_))\n",
    "print(\"\")\n",
    "print(\"Parameters for PENALIZED LOGISTIC REGRESSION\")\n",
    "print(\"     KFold w/out Shuffling: {}\".format(pen_logreg_grid.best_params_))\n",
    "print(\"            Repeated KFold: {}\".format(pen_logreg_grid_rfk.best_params_))\n",
    "print(\"Repeated KFold w/ new seed: {}\".format(pen_logreg_grid_rfk_1.best_params_))\n",
    "print(\"\")\n",
    "print(\"Parameters for KNN CLASSIFIER\")\n",
    "print(\"     KFold w/out Shuffling: {}\".format(knn_grid.best_params_))\n",
    "print(\"            Repeated KFold: {}\".format(knn_grid_rfk.best_params_))\n",
    "print(\"Repeated KFold w/ new seed: {}\".format(knn_grid_rfk_1.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vc_k2dVS0G_U"
   },
   "source": [
    "**Answer:** Changing the random seed of the shuffling didn't change the recommended parameters for any of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_yavLi20G_W"
   },
   "source": [
    "## Or if you change the random state of the split into training and test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q36hwti50G_Y"
   },
   "outputs": [],
   "source": [
    "# Resplit the data using a new random_state, 1000 (instead of 42)\n",
    "X_train_newsplit, X_test_newsplit, y_train_newsplit, y_test_newsplit = train_test_split(X, y, random_state=1000)\n",
    "\n",
    "# Scale new split data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train_newsplit)\n",
    "X_train_scaled_newsplit = scaler.transform(X_train_newsplit)\n",
    "X_test_scaled_newsplit = scaler.transform(X_test_newsplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APZGbacC0G_b"
   },
   "source": [
    "### GridsearchCV of Logistic Regression with... \n",
    "* Repeated KFold (rkf)\n",
    "* Random Seed = 1\n",
    "* New random split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4vWl64ub0G_d",
    "outputId": "0bc14d3e-f796-4e7c-d1c8-9e588d14e2a9"
   },
   "outputs": [],
   "source": [
    "logreg_grid_rfk_1_newsplit = GridSearchCV(logreg_pipe, logreg_param_grid, cv=rkf_1).fit(X_train_scaled_newsplit, y_train_newsplit)\n",
    "\n",
    "print(\"LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(logreg_grid_rfk_1.score(X_test_scaled_newsplit, y_test_newsplit)))\n",
    "print(\"Best Parameter: {}\".format(logreg_grid_rfk_1_newsplit.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rr68Sw3C0G_3"
   },
   "source": [
    "### GridsearchCV of Penalized Logistic Regression with...\n",
    "* Repeated KFold (rkf)\n",
    "* Random Seed = 1\n",
    "* New random split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Va1esxGm0G_5",
    "outputId": "bad72e5d-8727-4c8c-e269-f7df4ce1456b"
   },
   "outputs": [],
   "source": [
    "pen_logreg_grid_rfk_1_newsplit = GridSearchCV(pen_logreg_pipe, pen_logreg_param_grid, cv=rkf_1).fit(X_train_scaled_newsplit, y_train_newsplit)\n",
    "\n",
    "print(\"PENLIZED LOGISTIC REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(pen_logreg_grid_rfk_1_newsplit.score(X_test_scaled_newsplit, y_test_newsplit)))\n",
    "print(\"Best Parameter: {}\".format(pen_logreg_grid_rfk_1_newsplit.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPvX1J-b0G_9"
   },
   "source": [
    "### GridsearchCV of KNN Classifier with...\n",
    "* Repeated KFold (rkf)\n",
    "* Random Seed = 1\n",
    "* New random split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "06g65IIv0G__",
    "outputId": "51188069-5217-4f9c-a8fd-3157e5e5c4ae"
   },
   "outputs": [],
   "source": [
    "knn_grid_rfk_1_newsplit = GridSearchCV(knn_pipe, knn_param_grid, cv=rkf_1).fit(X_train_scaled_newsplit, y_train_newsplit)\n",
    "\n",
    "print(\"KNN for REGRESSION (SCALED DATA)\")\n",
    "print(\"Test set Score: {:.2f}\".format(knn_grid_rfk_1_newsplit.score(X_test_scaled_newsplit, y_test_newsplit)))\n",
    "print(\"Best Parameter: {}\".format(knn_grid_rfk_1_newsplit.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOv9Q0G80HAD",
    "outputId": "fef2363b-8855-441e-af89-926603e36574"
   },
   "outputs": [],
   "source": [
    "# Comparing parameters with new random seed and newly split training & testing data\n",
    "print(\"Parameters for LOGISTIC REGRESSION\")\n",
    "print(\"                      KFold w/out Shuffling: {}\".format(logreg_grid.best_params_))\n",
    "print(\"                             Repeated KFold: {}\".format(logreg_grid_rfk.best_params_))\n",
    "print(\"                 Repeated KFold w/ new seed: {}\".format(logreg_grid_rfk_1.best_params_))\n",
    "print(\"Repeated KFold w/ new seed & new data split: {}\".format(logreg_grid_rfk_1_newsplit.best_params_))\n",
    "print(\"\")\n",
    "print(\"Parameters for PENALIZED LOGISTIC REGRESSION\")\n",
    "print(\"                      KFold w/out Shuffling: {}\".format(pen_logreg_grid.best_params_))\n",
    "print(\"                             Repeated KFold: {}\".format(pen_logreg_grid_rfk.best_params_))\n",
    "print(\"                 Repeated KFold w/ new seed: {}\".format(pen_logreg_grid_rfk_1.best_params_))\n",
    "print(\"Repeated KFold w/ new seed & new data split: {}\".format(pen_logreg_grid_rfk_1_newsplit.best_params_))\n",
    "print(\"\")\n",
    "print(\"Parameters for KNN CLASSIFIER\")\n",
    "print(\"                      KFold w/out Shuffling: {}\".format(knn_grid.best_params_))\n",
    "print(\"                             Repeated KFold: {}\".format(knn_grid_rfk.best_params_))\n",
    "print(\"                 Repeated KFold w/ new seed: {}\".format(knn_grid_rfk_1.best_params_))\n",
    "print(\"Repeated KFold w/ new seed & new data split: {}\".format(knn_grid_rfk_1_newsplit.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "unP3MkeO0HAI"
   },
   "source": [
    "**Answer:** Re-splitting the data into new training and new test data with a different random state changes the recommended parameters for all three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ymce0m7s0HAK"
   },
   "source": [
    "## 2.5 Lastly, compare the coefficients for Logistic Regression and Penalized Logistic Regression and discuss which final model you would choose to predict new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7TB_J2y0HAL",
    "outputId": "64f0bfc4-e486-4241-9dfc-49a4543d9249"
   },
   "outputs": [],
   "source": [
    "# This dataframe is built using the c3 object that was created earlier in the code\n",
    "coefs = pd.concat(c3)\n",
    "coefs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o-S3dmYb0HAP"
   },
   "source": [
    "**Note 1:** Scaling the data really minimizes the variation between the coefficients. Notice how coefficients of 'chlorides' are very different for Logistic Regression and Penalized Logistic Regression, but then when the data is scaled this difference is almost entirely minimized. Before scaling, 'chlorides' seemed like an important variable; after scaling, not so much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRxL4x9C0HAR"
   },
   "source": [
    "**Note 2:** The scaled data is also a little easier to interpret. After scaling, both the Logistic Regression and Penalized Logistic Regression models agree that 'volatile acidity', 'density', and 'alcohol' are important variables / features in determining 'winetype'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rgg2ypKs0HAS"
   },
   "source": [
    "**Answer:** I would use Penalized Logistic Regression (L2 penalty) with scaled data to predict new data.  It has the added benefit of highlighting important varaibles. (Though several models predicted cross validated data and test data equally well such that students could justify choosing other models)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW2_Model(1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
