{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Build a classification model using text data\n",
    "In part one of the homework, you will solve a text classification task.\n",
    "You can download the following data-sets from the HW data folder on the course\n",
    "website:\n",
    "HW4_Text_train_data.csv and HW4_text_test_data.csv\n",
    "The data consists of Women’s fashion online shop reviews, consisting of a review\n",
    "text, and whether the review author would recommend the product.\n",
    "We are trying to determine whether a reviewer will recommend a product or not based\n",
    "on each review.\n",
    "In a real application this might allow us to find out what is good or bad about certain\n",
    "products or to feature more typical reviews (like a very critical and a very positive one).\n",
    "Use cross-validation to evaluate the results. Use a metric that’s appropriate\n",
    "for imbalanced classification (AUC or average precision for example), and inspect all\n",
    "models by visualizing the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import data\n",
    "df_test = pd.read_csv('HW4_Text_test_data.csv')\n",
    "df_train = pd.read_csv('HW4_Text_train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check the data format\n",
    "print(df_train.shape)\n",
    "print(df_train.head())\n",
    "set(df_train.iloc[:, 1]) ##binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X = df_train.iloc[:, 0]\n",
    "df_train_y = df_train.iloc[:, 1]\n",
    "\n",
    "df_test_X = df_test.iloc[:, 0]\n",
    "df_test_y = df_test.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_X.shape)\n",
    "print(df_train_y.shape)\n",
    "\n",
    "print(df_test_X.shape)\n",
    "print(df_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train_y.value_counts())\n",
    "print(df_test_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_coefficients(coefficients, feature_names, n_top_features=25):\n",
    "    \"\"\"Visualize coefficients of a linear model.\n",
    "    Parameters\n",
    "    ----------\n",
    "    coefficients : nd-array, shape (n_features,)\n",
    "        Model coefficients.\n",
    "    feature_names : list or nd-array of strings, shape (n_features,)\n",
    "        Feature names for labeling the coefficients.\n",
    "    n_top_features : int, default=25\n",
    "        How many features to show. The function will show the largest (most\n",
    "        positive) and smallest (most negative)  n_top_features coefficients,\n",
    "        for a total of 2 * n_top_features coefficients.\n",
    "    \"\"\"\n",
    "    coefficients = coefficients.squeeze()\n",
    "    if coefficients.ndim > 1:\n",
    "        # this is not a row or column vector\n",
    "        raise ValueError(\"coeffients must be 1d array or column vector, got\"\n",
    "                         \" shape {}\".format(coefficients.shape))\n",
    "    coefficients = coefficients.ravel()\n",
    "\n",
    "    if len(coefficients) != len(feature_names):\n",
    "        raise ValueError(\"Number of coefficients {} doesn't match number of\"\n",
    "                         \"feature names {}.\".format(len(coefficients),\n",
    "                                                    len(feature_names)))\n",
    "    # get coefficients with large absolute values\n",
    "    coef = coefficients.ravel()\n",
    "    positive_coefficients = np.argsort(coef)[-n_top_features:]\n",
    "    negative_coefficients = np.argsort(coef)[:n_top_features]\n",
    "    interesting_coefficients = np.hstack([negative_coefficients,\n",
    "                                          positive_coefficients])\n",
    "    # plot them\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    colors = ['#ff2020' if c < 0 else '#0000aa'\n",
    "              for c in coef[interesting_coefficients]]\n",
    "    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients],\n",
    "            color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    plt.xticks(np.arange(1, 1 + 2 * n_top_features),\n",
    "               feature_names[interesting_coefficients], rotation=60,\n",
    "               ha=\"right\")\n",
    "    plt.ylabel(\"Coefficient magnitude\")\n",
    "    plt.xlabel(\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 1: countvectorize with logistic regression (not penalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###model 1: countvectorize with logistic regression (not penalized)\n",
    "bow_pipeline1 = Pipeline(steps=[('vect', CountVectorizer(stop_words='english')),\n",
    "                                ('LR', LogisticRegression())])\n",
    "\n",
    "param1 = {'vect__min_df':[1, 2, 3, 4, 5],\n",
    "          'LR__C':[0.01, 1, 10, 100]\n",
    "         }\n",
    "\n",
    "grid1 = GridSearchCV(bow_pipeline1, param_grid=param1, cv=cv, scoring='roc_auc')\n",
    "\n",
    "grid1.fit(df_train_X, df_train_y)\n",
    "\n",
    "print('Model 1 GridSearchCV Best Parameters: ', grid1.best_params_)\n",
    "print('Model 1 GridSearchCV Best Score:{:4f}'.format(grid1.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 1 GridSearchCV Test Score:{:4f}'.format(grid1.score(df_test_X, df_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = grid1.predict(df_test_X)\n",
    "print('Model 1 Metric Using Logistic Regression and Countvectorizer:')\n",
    "print(classification_report(df_test_y, y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 1 Visualization:')\n",
    "visualize_coefficients(grid1.best_estimator_.named_steps['LR'].coef_, grid1.best_estimator_.named_steps['vect'].get_feature_names(), n_top_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 2: tfidf with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "bow_pipeline2 = Pipeline(steps = [('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "                                 ('LR', LogisticRegression())])\n",
    "\n",
    "param2 = {'tfidf__min_df':[1, 2],\n",
    "          'tfidf__norm': ['l1', 'l2', None],\n",
    "         #'tfidf__ngram_range':[(1, 1), (1, 2), (1, 3)],\n",
    "          'LR__C': [0.01, 0.1, 1, 10, 100]   \n",
    "}\n",
    "\n",
    "grid2 = GridSearchCV(bow_pipeline2, param_grid=param2, cv=cv, scoring = \"roc_auc\")\n",
    "\n",
    "grid2.fit(df_train_X, df_train_y)\n",
    "\n",
    "print('Model 2 GridSearchCV Best Parameters: ', grid2.best_params_)\n",
    "print('Model 2 GridSearchCV Best Score:{:4f}'.format(grid2.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 2 GridSearchCV Test Score:{:4f}'.format(grid2.score(df_test_X, df_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow_pipeline.fit(df_train_X, df_train_y)\n",
    "#tf_cv = cross_val_score(bow_pipeline, df_train_X, df_train_y, cv=10, scoring='f1_macro')\n",
    "#print('Mean Cross-Validation Accuracy on Train Data:{: 4f}'. format(np.mean(tf_cv)))\n",
    "#print('Mean Cross-Validation Accuracy on Test Data:{: 4f}'. format(np.mean(cross_val_score(bow_pipeline, df_test_X, df_test_y, cv=10, scoring='f1_macro'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = grid2.predict(df_test_X)\n",
    "print('Model 2 Metric Using Logistic Regression and TFIDF:')\n",
    "print(classification_report(df_test_y, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 2 Visualization:')\n",
    "visualize_coefficients(grid2.best_estimator_.named_steps['LR'].coef_, grid2.best_estimator_.named_steps['tfidf'].get_feature_names(), n_top_features=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL 3: tfidf with Penalized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pipeline3 = Pipeline(steps=[('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "                                ('LR', LogisticRegression())])\n",
    "\n",
    "param3 = {'tfidf__min_df':[1, 2],\n",
    "          'tfidf__ngram_range':[(1, 1), (1, 2), (1, 3)],\n",
    "          'tfidf__norm': ['l1', 'l2', None],\n",
    "          'LR__C': [0.01, 0.1, 1, 10, 100],\n",
    "          'LR__penalty': ['l1', 'l2']}\n",
    "\n",
    "grid3 = GridSearchCV(bow_pipeline3, param_grid=param3, cv=cv, scoring='roc_auc')\n",
    "\n",
    "grid3.fit(df_train_X, df_train_y)\n",
    "\n",
    "print('Model 3 GridSearchCV Best Parameters: ', grid3.best_params_)\n",
    "print('Model 3 GridSearchCV Best Score:{:4f}'.format(grid3.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 3 GridSearchCV Test Score:{:4f}'.format(grid3.score(df_test_X, df_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred3 = grid3.predict(df_test_X)\n",
    "print('Model 3 Metric Using Logistic Regression and TFIDF:')\n",
    "print(classification_report(df_test_y, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 3 Visualization:')\n",
    "visualize_coefficients(grid3.best_estimator_.named_steps['LR'].coef_, grid3.best_estimator_.named_steps['tfidf'].get_feature_names(), n_top_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL 4: tfidf with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#bow_pipeline4 = Pipeline(steps=[('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "#                               ('DT', DecisionTreeClassifier())])\n",
    "\n",
    "#param4 = {'tfidf__min_df':[1, 2],\n",
    "#         'tfidf__ngram_range': [(1, 1), (1, 2), (2, 3)],\n",
    "#         'DT__criterion': ['gini', 'entropy'],\n",
    "#         'DT__max_depth': [4, 6, 8]\n",
    "#}\n",
    "\n",
    "#grid4 = GridSearchCV(bow_pipeline4, param_grid=param4, cv=5, scoring='roc_auc')\n",
    "\n",
    "#grid4.fit(df_train_X, df_train_y)\n",
    "\n",
    "#print('Model 4 GridSearchCV Best Parameters: ', grid4.best_params_)\n",
    "#print('Model 4 GridSearchCV Best Score:{:4f}'.format(grid4.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Model 4 GridSearchCV Test Score:{:4f}'.format(grid4.score(df_test_X, df_test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred4 = grid4.predict(df_test_X)\n",
    "\n",
    "#print('Model 4 Metric Using Decision Tree and TFIDF:')\n",
    "#print(classification_report(df_test_y, y_pred24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 Conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = classification_report(df_test_y, y_pred1, output_dict=True)\n",
    "b = classification_report(df_test_y, y_pred2, output_dict=True)\n",
    "c = classification_report(df_test_y, y_pred3, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_auc = [grid1.best_score_, grid2.best_score_, grid3.best_score_]\n",
    "test_auc =[grid1.score(df_test_X, df_test_y),grid2.score(df_test_X, df_test_y), grid3.score(df_test_X, df_test_y)]\n",
    "test_f1_macro = [a['macro avg']['f1-score'], b['macro avg']['f1-score'], c['macro avg']['f1-score']]\n",
    "test_f1_weighted = [a['weighted avg']['f1-score'], b['weighted avg']['f1-score'], c['weighted avg']['f1-score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {'Train_AUC': train_auc, \n",
    "          'Test_AUC': test_auc,\n",
    "          'Test F1_Macro': test_f1_macro,\n",
    "          'Test F1_Weighted': test_f1_weighted\n",
    "         }\n",
    "\n",
    "\n",
    "q1_result = pd.DataFrame(result, index=['model 1', 'model 2', 'model 3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(q1_result, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: choose model 3, penalized logistic regression (L2 penalty and C=10) with tfidf (min_df =1 and ngram (1,2))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Build a predictive neural network using Keras\n",
    "To complete part two of the homework do the following:\n",
    "Run a multilayer perceptron (feed forward neural network) with two hidden layers on\n",
    "the iris dataset using the keras Sequential interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import data\n",
    "df2 = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv', encoding = 'latin_1')\n",
    "df2 = df2.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.iloc[:, 0:4]\n",
    "y = df2.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "set(y)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = LabelEncoder()\n",
    "y_new = encode.fit_transform(y)\n",
    "y_new = keras.utils.to_categorical(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_new, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train_scale = sc.fit_transform(X_train)\n",
    "X_train_scale= pd.DataFrame(X_train_scale, columns=X_train.columns, index=X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scale = sc.transform(X_test)\n",
    "X_test_scale = pd.DataFrame(X_test_scale, index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def create_dummies(df,column_name):\n",
    "    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n",
    "    df = pd.concat([df,dummies],axis=1)\n",
    "    return df\n",
    "data = create_dummies(df2,\"Species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model(learn_rate=0.01, act='sigmoid', n =10):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n, input_dim=4, activation=act))\n",
    "    model.add(Dense(n, activation=act))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    optimizer = SGD(lr=learn_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = create_model()\n",
    "base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "learn_rate = [0.001, 0.01]\n",
    "act = ['sigmoid', 'relu', 'softmax']\n",
    "n = [10, 50, 100]\n",
    "epochs = [10, 20, 30]\n",
    "batch = [5, 10, 50]\n",
    "\n",
    "param = dict(learn_rate=learn_rate, act = act, n = n, epochs = epochs, batch_size = batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param, n_jobs=-1)\n",
    "\n",
    "nn_model = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best: %f using %s\" % (nn_model.best_score_, nn_model.best_params_))\n",
    "#means = nn_model.cv_results_['mean_test_score']\n",
    "#stds = nn_model.cv_results_['std_test_score']\n",
    "#params = nn_model.cv_results_['params']\n",
    "#for mean, stdev, param in zip(means, stds, params):\n",
    "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Neural Network Best Paramter: %s' % nn_model.best_params_)\n",
    "print('Neural Network Best Score: %f' % nn_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Neural Network Test Set: %f' % nn_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Model with New Parameters\n",
    "best_model = Sequential() # Best Parameter\n",
    "best_model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "best_model.add(Dense(10, activation='relu'))\n",
    "best_model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile Model\n",
    "optimizer = SGD(lr=0.01) # Best Parameter\n",
    "best_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Fit the Model\n",
    "best_model.fit(X_train, y_train,  epochs=20, batch_size=5) # Best Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list()\n",
    "for i in range(len(y_pred)):\n",
    "    pred.append(np.argmax(y_pred[i]))\n",
    "    \n",
    "test = list()\n",
    "for i in range(len(y_test)):\n",
    "    test.append(np.argmax(y_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "test_score = accuracy_score(pred, test)\n",
    "print('Neural Network Accuracy is:', round(test_score, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
