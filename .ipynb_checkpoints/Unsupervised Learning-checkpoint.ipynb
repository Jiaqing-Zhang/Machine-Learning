{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "# make your plot outputs appear and be stored within the notebook\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "dfred= pd.read_csv(\"winequality-red.csv\", sep=';')\n",
    "print(dfred.head(10))\n",
    "dfred.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the dataset to give them all equal importance. Scaling is very important for a clustering analysis as the distance between points affects the way cluster are formed.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.DataFrame(StandardScaler().fit_transform(dfred), index=dfred.index, columns=dfred.columns)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfred['quality'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quality'].describe() # the range of transformed quality is from -3.27 to 2.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##correlation plot \n",
    "corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot source \"https://seaborn.pydata.org/examples/many_pairwise_correlations.html\"\n",
    "\n",
    "sns.set(style='white')\n",
    "\n",
    "#generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "#set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "#generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "#draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the corrolation plot, we could find that the quality is highly positively correlated with alcohol; and negatively correlated with density and volatile acidity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use K Means Cluster Analysis to identify cluster(s) of observations that have high and low values of the wine quality. (Assume all variables are continuous.) Describe variables that cluster with higher values of wine quality. Describe variables that cluster with lower values of wine quality. If you want to make a good bottle of wine, then what characteristics are most important according to this analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"https://towardsdatascience.com/clustering-with-k-means-1e07a8bfb7ca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the elbow plot to choose the number of clusters\n",
    "\n",
    "with_cluster_variation = []\n",
    "\n",
    "for k in range(2, 12):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(df)\n",
    "    with_cluster_variation.append(kmeans.inertia_)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.plot(range(2, 12), with_cluster_variation, marker = 'o')\n",
    "plt.grid(True) #create the grid in figure\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Reduction in within cluster variation')\n",
    "plt.title('Elbow Plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no clear elbow on this plot. But I think 7 clusters may be reasonable as after 7 clusters, the variation deduction trend slows down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_centers(cols_of_interest, centers):\n",
    "    colNames = list(cols_of_interest)\n",
    "    colNames.append('prediction')\n",
    "    \n",
    "    #zip with a column called 'prediction' (index)\n",
    "    Z = [np.append(A, index) for index, A in enumerate(centers)]\n",
    "    \n",
    "    #convert to pandas dataframe for plotting\n",
    "    p = pd.DataFrame(Z, columns=colNames)\n",
    "    p['prediction'] = p['prediction'].astype(int)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use 7 clusters\n",
    "kmean_7 = KMeans(n_clusters=7).fit(df)\n",
    "kmean_result = pd_centers(df.columns, kmean_7.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_7_labels = kmean_7.predict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(kmean_7_labels) # the unique label is from 0 to 6, which means there is 7 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.values[kmean_7_labels==0, 0], df.values[kmean_7_labels==0, 1], s=50, marker='o', color='red', label='cluster 1')\n",
    "plt.scatter(df.values[kmean_7_labels==1, 0], df.values[kmean_7_labels==1, 1], s=50, marker='o', color='blue', label = 'cluster 2')\n",
    "plt.scatter(df.values[kmean_7_labels==2, 0], df.values[kmean_7_labels==2, 1], s=50, marker='o', color='green', label = 'cluster 3')\n",
    "plt.scatter(df.values[kmean_7_labels==3, 0], df.values[kmean_7_labels==3, 1], s=50, marker='o', color='purple', label = 'cluster 4')\n",
    "plt.scatter(df.values[kmean_7_labels==4, 0], df.values[kmean_7_labels==4, 1], s=50, marker='o', color='orange', label = 'cluster 5')\n",
    "plt.scatter(df.values[kmean_7_labels==5, 0], df.values[kmean_7_labels==5, 1], s=50, marker='o', color='gray', label = 'cluster 6')\n",
    "plt.scatter(df.values[kmean_7_labels==6, 0], df.values[kmean_7_labels==6, 1], s=50, marker='o', color='yellow', label = 'cluster 7')\n",
    "plt.legend()\n",
    "plt.title('KMeans Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we could find that:\n",
    "\n",
    "1. high quality positively associtated with alcohol;\n",
    "2. high value of density associate with low quality; low density associate with high quality\n",
    "3. high value of volatile acidity associate with low quality; low volatile acidity associate with high quality\n",
    "\n",
    "Thus, I think alcohol, density and volatile acidity may associated with quality of wine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use Hierarchical Cluster Analysis to identify cluster(s) of observations that have high and low values of the wine quality. (Assume all variables are continuous.) Use complete linkage and the same number of groups that you found to be the most meaningful in question 1. Describe variables that cluster with higher values of wine quality. Describe variables that cluster with lower values of wine quality. If you want to make a good bottle of wine, then what characteristics are most important according to this analysis? Have your conclusions changed using Hierarchical clustering rather than k means clustering? Present any figures that assist you in your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/machine-learning-algorithms-part-12-hierarchical-agglomerative-clustering-example-in-python-1e18e0075019\n",
    "\n",
    "https://towardsdatascience.com/https-towardsdatascience-com-hierarchical-clustering-6f3c98c9d0ca\n",
    "\n",
    "### two types of hierarchical clustering algorithms\n",
    "agglomerative: bottom up approach; start with many small clusters and merge them together to create bigger clusters;\n",
    "divisive: top down approach; start with a single cluster than break it up into smaller clusters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create adjacency matrix and pdist applies Euclidean distance to each combo of observation\n",
    "#row_dist = pd.DataFrame(squareform(pdist(df, metric='euclidean')), columns= df.columns)\n",
    "#row_dist\n",
    "\n",
    "#linkage criteria refers to how the distance between clusters is calculated\n",
    "\n",
    "row_clusters = sch.linkage(pdist(df, metric = 'euclidean'), method = 'complete') # define distance metric and linkage for model\n",
    "# complete linkage: the distance between two clusters is the longest distance between two points in each cluster.\n",
    "# single linkage: the distance between two clusters is the shortest distance between two points in each cluster.\n",
    "# average linkage: the distance between clusters is the average distance between each point in one cluster ti every point in other cluster.\n",
    "# ward linkage: the distance between clusters is the sum of squared differences within all clusters; minimize the variant between clusters.\n",
    "# row_clusters has meta-data we can use to visulize the HC fit with a dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dendrogram on complete linkage\n",
    "dendrogram_complete = sch.dendrogram(row_clusters)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Euclidean Distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dendrogram on ward linkage\n",
    "dendrogram_wald = sch.dendrogram(sch.linkage(df.values, method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is hard to determine the number of clusters using the complete linkage. However, the ward linkage shows that there is seven clusters, which is the same results as the kmean method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_model = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='complete')\n",
    "hc_model.fit(df)\n",
    "labels = hc_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(labels) # the unique label is from 0 to 6, which means there is 7 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hc = df\n",
    "df_hc['labels'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_result = df_hc.groupby('labels').mean()\n",
    "hc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.values[labels==0, 0], df.values[labels==0, 1], s=50, marker='o', color='red', label='cluster 1')\n",
    "plt.scatter(df.values[labels==1, 0], df.values[labels==1, 1], s=50, marker='o', color='blue', label = 'cluster 2')\n",
    "plt.scatter(df.values[labels==2, 0], df.values[labels==2, 1], s=50, marker='o', color='green', label = 'cluster 3')\n",
    "plt.scatter(df.values[labels==3, 0], df.values[labels==3, 1], s=50, marker='o', color='purple', label = 'cluster 4')\n",
    "plt.scatter(df.values[labels==4, 0], df.values[labels==4, 1], s=50, marker='o', color='orange', label = 'cluster 5')\n",
    "plt.scatter(df.values[labels==5, 0], df.values[labels==5, 1], s=50, marker='o', color='gray', label = 'cluster 6')\n",
    "plt.scatter(df.values[labels==6, 0], df.values[labels==6, 1], s=50, marker='o', color='yellow', label = 'cluster 7')\n",
    "plt.legend()\n",
    "plt.title('Hierachical Clustering')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs =plt.subplots(nrows=1, ncols=2, figsize=(15,15))\n",
    "\n",
    "axs[0].scatter(df.values[kmean_7_labels==0, 0], df.values[kmean_7_labels==0, 1], s=50, marker='o', color='red', label='cluster 1')\n",
    "axs[0].scatter(df.values[kmean_7_labels==1, 0], df.values[kmean_7_labels==1, 1], s=50, marker='o', color='blue', label = 'cluster 2')\n",
    "axs[0].scatter(df.values[kmean_7_labels==2, 0], df.values[kmean_7_labels==2, 1], s=50, marker='o', color='green', label = 'cluster 3')\n",
    "axs[0].scatter(df.values[kmean_7_labels==3, 0], df.values[kmean_7_labels==3, 1], s=50, marker='o', color='purple', label = 'cluster 4')\n",
    "axs[0].scatter(df.values[kmean_7_labels==4, 0], df.values[kmean_7_labels==4, 1], s=50, marker='o', color='orange', label = 'cluster 5')\n",
    "axs[0].scatter(df.values[kmean_7_labels==5, 0], df.values[kmean_7_labels==5, 1], s=50, marker='o', color='gray', label = 'cluster 6')\n",
    "axs[0].scatter(df.values[kmean_7_labels==6, 0], df.values[kmean_7_labels==6, 1], s=50, marker='o', color='yellow', label = 'cluster 7')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('KMeans Clustering')\n",
    "axs[1].scatter(df.values[labels==0, 0], df.values[labels==0, 1], s=50, marker='o', color='red', label='cluster 1')\n",
    "axs[1].scatter(df.values[labels==1, 0], df.values[labels==1, 1], s=50, marker='o', color='blue', label = 'cluster 2')\n",
    "axs[1].scatter(df.values[labels==2, 0], df.values[labels==2, 1], s=50, marker='o', color='green', label = 'cluster 3')\n",
    "axs[1].scatter(df.values[labels==3, 0], df.values[labels==3, 1], s=50, marker='o', color='purple', label = 'cluster 4')\n",
    "axs[1].scatter(df.values[labels==4, 0], df.values[labels==4, 1], s=50, marker='o', color='orange', label = 'cluster 5')\n",
    "axs[1].scatter(df.values[labels==5, 0], df.values[labels==5, 1], s=50, marker='o', color='gray', label = 'cluster 6')\n",
    "axs[1].scatter(df.values[labels==6, 0], df.values[labels==6, 1], s=50, marker='o', color='yellow', label = 'cluster 7')\n",
    "axs[1].legend()\n",
    "axs[1].set_title('Hierachical Clustering')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from kmean clustering and hierarchical clustering (ward) is almost the same. However, the complete linkage leads to slight different clusters compared with kmean results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we could find that:\n",
    "\n",
    "1. high quality positively associtated with alcohol;\n",
    "2. high value of sulphates associate with low quality; low sulphates associate with high quality;\n",
    "3. high value of chlorides associate with low quality; low chlorides associate with high quality;\n",
    "\n",
    "Thus, I think alcohol, sulphates and chlorides may associated with quality of wine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Principal Components Analysis to reduce the dimensions of your data. How much of the variation in your data is explained by the first two principal components. How might you use the first two components to do supervised learning on some other variable tied to wine (e.g. - wine price)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot determine the number of components\n",
    "pca = PCA().fit(df)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.grid(True)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's choose the number of components that explain how much of the variance\n",
    "print(\"To explain 80% of the variation of the data, we need \" + str(PCA(0.80).fit(df).n_components_ )+ \" components\") #6 components should be chose\n",
    "print(\"To explain 85% of the variation of the data, we need \" + str(PCA(0.85).fit(df).n_components_ )+ \" components\") #7 components should be chose\n",
    "print(\"To explain 90% of the variation of the data, we need \" + str(PCA(0.90).fit(df).n_components_ )+ \" components\") #9 components should be chose\n",
    "print(\"To explain 95% of the variation of the data, we need \" + str(PCA(0.95).fit(df).n_components_ )+ \" components\") #10 components should be chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how many variance explained by 2 components \"{:.0f}\".format(x))\n",
    "print(\"Variation explained by first two components: \"+ \"{:.0f}\".format(100*sum(PCA(n_components=2).fit(df).explained_variance_ratio_))+\"%\")\n",
    "## the first two components explain about 44% variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=2).fit(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_2 = pca_2.transform(df)\n",
    "components_2\n",
    "len(components_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By deducting the dimension of 12 features to 2 components, we can then use the transformed components to fit a linear regression model to predict the outcome price. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
